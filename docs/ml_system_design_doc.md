# Документ для разработки MVP: Персонализированные рекомендации образовательных курсов

---

## Термины и пояснения

- **Итерация** — совокупность всех работ, проводимых до старта очередного пилота.  
- **БТ (бизнес-требования)** — требования, определяющие основные цели проекта и ограничения.  
- **EDA (Exploratory Data Analysis)** — исследовательский анализ данных, направленный на изучение и предварительную обработку данных.  
- **Роли**:  
  - **Product Owner** — отвечает за постановку бизнес-целей, разработку требований и оценку успеха.  
  - **Data Scientist** — совмещает функции исследователя данных, ML-инженера и ML Ops-специалиста для разработки и продуктивизации моделей.  
- Распределение ролей может варьироваться в зависимости от операционной модели вашей организации.

---

## 1. Цели и предпосылки

### 1.1. Зачем идем в разработку продукта?

#### Бизнес-цель (Product Owner):  
Привлечь аудиторию на платформу образовательных курсов с помощью инструмента, который подбирает пользователю курс на основе требований вакансии и текущего опыта пользователя. Для этого будут использованы инструменты NLP и классического ML для мэтчинга скиллов вакансий с курсами и их стеком, а также другими сущностями. Это позволит повысить интерес аудитории и, в перспективе, привлечь рекламу и монетизировать платформу.

#### Почему станет лучше (Product Owner & Data Scientist):  
- Повысится вовлечённость пользователей за счёт персонализированных рекомендаций, основанных на мэтчинге скиллов вакансий и стека курсов.
- Платформа станет инструментом карьерного роста, что усилит её конкурентоспособность.  
- Алгоритм подстраивается под интерес пользователя к конкретной вакансии, предоставляя наиболее релевантный курс для освоения необходимых скиллов, исходя из схожести требований вакансии.
- Бизнес получит возможность демонстрировать аудитории актуальные курсы, стимулируя спрос.  

#### Успех итерации с точки зрения бизнеса (Product Owner):  
- Улучшение вовлечённости лидов и повышение их интереса к платформе.  
- Высокая удовлетворённость пользователей предложенными рекомендациями.  

---

### 1.2. Бизнес-требования и ограничения

#### Краткое описание БТ (Product Owner):  
- Автоматизация анализа вакансий с сайтов вакансий.  
- Генерация релевантных рекомендаций образовательных курсов на основе сходства навыков в вакансии и резюме.  
 

#### Бизнес-ограничения (Product Owner):  
- Ограничение на бюджет MVP.  
- Максимальная скорость обработки запросов.  

#### Ожидания от итерации (Product Owner):  
- Завершение тестирования MVP.  
- Подготовка базы для масштабирования функционала и интеграции рекламы.  

#### Описание бизнес-процесса пилота (Product Owner):  
- Пользователь отправляет ссылку вакансии и своего резюме → система анализирует требования → выдаёт список подходящих курсов с платформы.  
- На основе рекомендаций отслеживается поведение пользователей (например, переходы на курсы).  

#### Успех пилота (Product Owner):  
- Успешное тестирование рекомендаций с валидированной точностью.  
- Готовность функционала систем для интеграции курсов разных образовательных платформ.  
- Подготовка к масштабированию решения на дополнительные источники данных.  

---

### 1.3. Что входит в скоуп проекта/итерации, что не входит

#### Входит (Data Scientist):  
- Построение базовой модели анализа вакансий.  
- Разработка MVP для тестирования рекомендаций.  
- Документирование всех процессов для дальнейшего развития.  

#### Не входит (Data Scientist):  
- Полная интеграция рекламы (в случае если проект будет интересен рекламодателям (образовательные платформы)).  
- Детальная настройка пользовательского интерфейса.  

#### Описание результата (Data Scientist):  
- Воспроизводимое решение, готовое для масштабирования.  

#### Технический долг (Data Scientist):  
- Оптимизация производительности.  
- Разработка модулей для расширения функционала.  

---

### 1.4. Предпосылки решения

#### Общие предпосылки (Data Scientist):  
- Данные: текст вакансий (HH.ru, superjob и тд) и описание курсов (GeekBrains).  
- Горизонт прогноза: мгновенная выдача рекомендаций.  
- Гранулярность модели: на уровне отдельного пользователя.  

---

## 2. Методология

### 2.1. Постановка задачи (Data Scientist)  
- Построение рекомендательной системы для образовательной платформы.  
- Анализ требований вакансий и сопоставление их с описанием курсов.  

---

### 2.2. Блок-схема решения (Data Scientist)  

#### Основные этапы:  
1. **Сбор данных**:  
   - Извлечение вакансий с сайта вакансий.  
   - Сбор информации о курсах с платформы GeekBrains.  

2. **Обработка данных**:  
   - Исследовательский анализ данных (EDA).  
   - Преобразование текстов вакансий и курсов (токенизация, векторизация).  

3. **Модель**:  
   - NLP-алгоритмы для анализа текста.  
   - Рекомендательная система на основе сопоставления данных.  

4. **Тестирование**:  
   - Проверка качества рекомендаций (точность, релевантность).  
   - Сбор обратной связи от пользователей.  

5. **Закрытие технического долга**:  
   - Оптимизация кода.  
   - Подготовка к интеграции рекламного модуля.  
6. Блок схема:
![Схема работы программы](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/sheme.jpg)


---

### Этапы решения задачи

# Этап 1.1 EDA и Подготовка данных

### Описание данных/сущностей

Изначально данные собирались вручную с платформ HH.ru, superjob, rvr, rabota.ru (тексты вакансий) и GeekBrains (описания курсов). Данные из различных источников включают информацию о вакансиях, курсах и скиллах.

#### Данные:
1. **data_it.csv**:
    - Включает **171,140 записей** с информацией о вакансиях.
    - **31 колонка** с различной информацией о вакансиях, включая:
        - **Общая информация**: Источник данных, идентификаторы вакансий, дата публикации, статус вакансии.
        - **Информация о работодателе**: Название компании, отрасль, тип работодателя.
        - **Характеристики вакансии**: Описание, зарплата, требуемый опыт, навыки (hard_skills, soft_skills).
        - **Географическая информация**: Страна, регион, район.
        - **Категория и роль**: Категория вакансии, роль.

![sheme](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/columns_info.png)

  #### Объем данных:
    - Данные с конца 2022 до конца 2023 года собирались вручную, что ограничивало масштаб анализа.
    - С увеличением объема данных сбор стал занимать больше времени и ресурсов.

![publications_per_mnth](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/publications_per_mnth.png)

  #### Качество данных:
    - Пропуски в колонках:
      - `region_district_name` (13%), `role_name` (6.7%), `salary_currency` (18.3%).
    - Разнородность формата навыков (`skills`, `hard_skills`, `soft_skills`).
    - Неконсистентность данных о зарплатах (разные валюты, неоднозначность значений "от" и "до").

![nan_info](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/nan_info.png)

![nans_visualize](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/nans_visualize.png)

![salary_distribution](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/salary_distribution.png)

Имеем такие распределения по источникам данных, типу занятости, необходимому опыту и топ 10 вакансиям.

![source_distribution](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/source_distribution.png)

![source_distribution](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/shedule_distribution.png)

![exp_distribution](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/exp_distribution.png)

![jobs_top_10](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/jobs_top_10.png)


---

2. **preprocess_it.csv**:
    - Данные, относящиеся исключительно к hard и soft скиллам. Содержат информацию о навыках, требуемых для различных вакансий.
    Где name - наименование вакансии,
    description - описание вакансии,
    hard_skills - прописанные скиллы для вакансии.

  ```plaintext
  <class 'pandas.core.frame.DataFrame'>
  RangeIndex: 170832 entries, 0 to 170831
  Data columns (total 4 columns):
  #   Column       Non-Null Count   Dtype 
  ---  ------       --------------   ----- 
  0   Unnamed: 0   170832 non-null  int64 
  1   name         170832 non-null  object
  2   description  170832 non-null  object
  3   hard_skills  81237 non-null   object
  dtypes: int64(1), object(3)
  ```

3. **df_hh_res_all.csv**:
    - Включает **69161 записей** и 361 колонку, где:
        - 5 колонок типа `object`: зарплата, график работы, опыт, занятость, регион.
        - Остальные колонки — скиллы с флагами 0 или 1, указывающими на наличие скилла.
!['salary_hh_all'](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/salary_hh_all.png)

!['exp_hh'](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/exp_hh.png)

!['zanyatost_hh'](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/zanyatost_hh.png)

!['regions_hh'](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/regions_hh.png)

!['nans_vis_hh'](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/nans_vis_hh.png)





4. **trebovanyia_full_data.csv**:
    - **1022 записи** с размеченными текстами вакансий, где указано, является ли текст связанным с скиллами (1 — скиллы, 0 — не скиллы).
    - Структура данных:
        - **text**: текст вакансии.
        - **label**: метка (1 — скиллы, 0 — не скиллы).
![trebovaniya_opisanya](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/trebovaniya_opisanya.png)
![trebovaniya_counts](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/trebovaniya_counts.png)


    Пример:
    ```plaintext
    RangeIndex: 1022 entries, 0 to 1021  
    Data columns (total 3 columns):
    #   Column      Non-Null Count  Dtype 
    ---  ------      --------------  ----- 
    0   Unnamed: 0  1022 non-null   int64 
    1   text        1022 non-null   object
    2   label       1022 non-null   int64 
    label
    1    525
    0    497
    Name: count, dtype: int64
    ```

---

### Процесс генерации данных

#### Источники данных:
- **HH.ru, superjob, rvr, rabota.ru**: вакансии собираются через ручные парсинг-скрипты, данные сохраняются в формате CSV.
- **GeekBrains**: информация о курсах загружается аналогичным образом.

#### Регулярность:
- Обновления данных были нерегулярными из-за ручного сбора. С увеличением объема данных потребуется автоматизировать процесс.

---

### Недостаточность данных

Для повышения качества рекомендаций курсов по вакансии необходимо:
- Расширить временной диапазон данных.
- Увеличить разнообразие вакансий и курсов, охватывающих больше отраслей и регионов.

---

### Риски:
- **Устаревшая информация в вакансиях по скиллам**.
- **Недостаточная детализация вакансий** (например, отсутствие описания роли).
- **Перекосы в данных** из-за различий в регионах, отраслях и компаниях.

---

# Этап 1.2: Классификация текста на наличие скиллов

## 1. Сбор и подготовка данных:
- **Датасет**: текстовые описания вакансий с лейблом, указывающим, является ли текст связанным с скиллами (1 - скиллы, 0 - не скиллы) из таблицы `trebovanyia_full_data.csv`.
- **Требования**: данные должны быть очищены от шума (стоп-слов, знаков препинания), текст должен быть приведен к единому виду (например, привести к нижнему регистру).

## 2. Предобработка текста:
- Приведение текста к нижнему регистру.
- Удаление стоп-слов, знаков препинания, чисел и ненужных символов.
- Лемматизация или стемминг для приведения слов к их базовой форме.
- **Требования**: обработка текста должна быть выполнена так, чтобы сохранялась максимальная смысловая нагрузка и исключались лишние элементы.

## 3. Векторизация текста:
- Преобразование текста в числовые данные с использованием **TF-IDF** (или **Word2Vec** для более сложных моделей).
- **Требования**: метод должен быть выбран таким образом, чтобы сохранять контекст и структуру текста, минимизируя размерность и избыточность.
- **Метрики**: **accuracy**, **precision**, **recall**, **F1-score**.



## 4. Обучение модели: 
- Обучение классификатора (например, **логистической регрессии**, **наивного байесовского классификатора** или **случайного леса**) для предсказания наличия скиллов в тексте.
- **Метрики**: **accuracy**, **precision**, **recall**, **F1-score**.

## 5. Оценка модели:
- Оценка точности модели с использованием вышеупомянутых метрик. Результаты должны быть интерпретированы, чтобы понять, насколько хорошо модель различает скиллы от не скиллов.

## **Бейзлайн для этапа 2**:
Простой классификатор (например, **наивный байесовский классификатор** или **логистическая регрессия**) с точностью более **80%** на тестовых данных.

## 6. Риски и способы их снижения:
 
- **Переобучение**: Для предотвращения переобучения используется регуляризация и кросс-валидация.

## 7. Необходимый результат этапа:  
Обученная модель классификации с точностью более **80%** на тестовых данных.

---

# Этап 1.3: Обработка вакансий и предсказание требований

## 1. Загрузка и подготовка модели и векторизатора:
- Загрузка обученной модели классификации и **TF-IDF** векторизатора.
- **Требования**: правильная загрузка модели и векторизатора, чтобы предсказания были основаны на ранее обученных данных.

## 2. Обработка данных вакансий:
- Извлечение данных с сайта вакансии (парсинг URL).
- Парсинг ключевых навыков, описания вакансии, требований и других данных.
- **Требования**: данные вакансии должны быть корректно извлечены и готовы для анализа.

## 3. Предсказание наличия требований:
- Преобразование текста вакансии в числовой формат с использованием **TF-IDF**.
- Применение обученной модели для предсказания наличия скиллов в вакансиях.
- **Метрики**: **accuracy**, **precision**, **recall**, **F1-score**.

## 4. Постобработка и добавление результатов:
- Применение модели для предсказания, является ли каждый текст вакансии скиллами (обученный классификатор). 
- Каждый текст вакансии обрабатывается для выделения ключевых навыков и добавления предсказанных меток (1 - скиллы, 0 - не скиллы).
- Добавление этих предсказанных меток в итоговый словарь вакансий.
- Для каждой вакансии можно вывести список навыков, которые модель считает важными, и представить их в отдельной колонке.

## 5. Метрики расстояния:
- Для сопоставления вакансий с курсами можно использовать различные метрики расстояния, такие как:
  - **Косинусное расстояние** (Cosine distance)
  - **Евклидово расстояние** (Euclidean distance)
  - **Манхэттенское расстояние** (Manhattan distance)
  - **Жаккардова мера** (Jaccard similarity)

## 6. Нормализация и сортировка данных:
- Сортировка вакансий по общей схожести с курсами, чтобы определить, какие курсы наиболее соответствуют требованиям вакансий.
- **Требования**: сортировка должна быть выполнена так, чтобы вакансии с наибольшим сходством с курсами шли первыми.


## **Бейзлайн для этапа 3**:
Вывод наиболее схожих с вакансией курсов на основе вычисленных метрик расстояния (например, с использованием **косинусного расстояния**). 

## 7. Риски и способы их снижения:
- **Ошибки в извлечении информации**: Использование более стабильных методов парсинга и строгих фильтров для извлечения данных вакансий.
- **Низкая точность предсказаний**: Оптимизация модели и предобработки для повышения точности предсказаний, а также использование более точных методов классификации, например, **градиентного бустинга**.

## 8. Необходимый результат этапа:  
Итоговый DataFrame с вакансиями, отсортированными по схожести с курсами, для каждой вакансии предлагаются соответствующие курсы с наибольшим сходством.
Пример в `utils.py `


# Этап 1.4: Обучение модели для предсказания увеличения заработной платы после прохождения курса

## Подготовка данных для обучения

1. **Данные для обучения**:  
   Для обучения модели будет использован датасет `df_hh_res_all.csv`, который содержит информацию о навыках, требуемых для вакансий, и средних зарплатах для каждой вакансии. Важно, что каждая запись в датасете включает информацию о навыках, требуемых для вакансии, а также о средней зарплате, которая предлагается кандидатам, соответствующим этим навыкам.

2. **Предобработка данных**:
   - Преобразование категориальных признаков (например, тип вакансии, требования и прочее) в числовые признаки с использованием one-hot encoding или порядкового кодирования.
   - Преобразование навыков вакансий в числовые векторы с использованием методов, таких как TF-IDF или Word2Vec.
   - Обработка пропусков и стандартизация данных (нормализация числовых признаков).

3. **Целевая переменная**:  
   Для каждой вакансии будет предсказана зарплата кандидата, который пройдет курс и получит новые навыки. Целевая переменная — это **зарплата кандидата**, которая будет зависеть от набора навыков, требуемых для вакансии, и дополнительных навыков, приобретенных после курса.

## Обучение модели

1. **Выбор модели**:  
   Модель будет обучена для предсказания зарплаты с учетом текущих навыков, требуемых для вакансии, и будущих навыков, которые кандидат может приобрести после прохождения курса. Для начального этапа обучения будет использована **линейная регрессия** как базовая модель. В дальнейшем, если точность модели окажется недостаточной, можно будет перейти к более сложным моделям, таким как **градиентный бустинг** (например, XGBoost) или **нейронные сети**.

2. **Метрики**:  
   Для оценки качества модели будут использованы следующие метрики:
   - **MSE (Mean Squared Error)**: Среднеквадратичная ошибка, которая будет измерять величину расхождения предсказанных значений зарплаты с реальными.
   - **MAE (Mean Absolute Error)**: Средняя абсолютная ошибка, которая дает представление о средней ошибке предсказания.

3. **Процесс обучения**:  
   - Обучение модели будет проводиться на данных, где каждой записи о вакансии соответствуют прогнозируемые значения зарплаты для кандидатов.
   - Для каждой вакансии будут созданы два прогноза: один для текущих навыков (предикт для навыков, указанных в резюме кандидата) и второй для прогнозируемых навыков после прохождения курса (предикт для дополнительных навыков).
   - Модель будет обучаться с использованием кросс-валидации для предотвращения переобучения.

## Интеграция модели в приложение

1. **Интеграция**:  
   После обучения модель будет интегрирована в приложение, которое будет предсказывать зарплату кандидата, учитывая как текущие навыки, требуемые для вакансии, так и дополнительные навыки, приобретенные после прохождения курса.
   
2. **Прирост зарплаты**:  
   Для каждого кандидата будет вычисляться прирост зарплаты путем вычитания предсказанных значений для текущих и будущих навыков. То есть, сначала будет сделан прогноз на основе текущих навыков кандидата, затем прогноз для дополнительных навыков, которые он получит после курса. Разница между этими двумя предсказаниями и будет составлять прирост в зарплате.

3. **Корректность интеграции**:  
   Важно обеспечить правильную интеграцию модели в приложение, чтобы она могла делать предсказания в реальном времени и обеспечивала стабильную работу.

## Анализ и интерпретация работы модели

1. **Методы интерпретации**:
   Для анализа важности признаков можно использовать такие методы, как **SHAP** или **LIME**, чтобы понять, какие навыки оказывают наибольшее влияние на прогнозируемую зарплату кандидата.

2. **Метрики интерпретации**:
   - **Ошибка модели**: Оценка точности предсказания.
   - **Влияние признаков**: Анализ того, какие именно навыки или характеристики вакансии влияют на увеличение зарплаты.

## Бейзлайн для этапа 1.4

- На первом этапе будет использоваться **линейная регрессия** как базовый метод для прогнозирования зарплаты, с точностью предсказания, достигающей **85%**. В случае неудовлетворительных результатов можно будет перейти к более сложным моделям, таким как **градиентный бустинг** или **нейронные сети**.

## Риски и способы их снижения

1. **Переобучение**:  
   Для предотвращения переобучения будет использована **кросс-валидация** и методы **регуляризации** (например, L1/L2-регуляризация).
   
2. **Низкая точность прогноза**:  
   Если точность модели окажется недостаточной, будет проведен переход к более сложным моделям, таким как **градиентный бустинг** или **нейронные сети**.

## Необходимый результат этапа

1. **Обученная и интегрированная модель**, которая предсказывает зарплату кандидата с текущими и будущими навыками и вычисляет прирост зарплаты.
2. **Интегрированная модель в приложение** с точностью прогноза **более 85%**.
3. **Отчёт о результатах предсказания** с интерпретацией значимости признаков, влияющих на зарплату, и прирост зарплаты после обучения.

# Конечный результат для пользователя:
- Если пользователь загружает вакансию, ему будет предложен подходящий курс для обучения.
- Если пользователь дополнительно загружает резюме, система рассчитает прирост в зарплате для наиболее подходящего курса, основываясь на навыках, указанных в резюме, и навыках, которые можно будет получить после прохождения курса.


# Этап 2. Подготовка прогнозных моделей

## Этап 2.1. Постановка задачи

### Цель:
Разработка системы рекомендаций на основе матчинга для выбора подходящих курсов, ориентированных на улучшение навыков, необходимых для конкретных вакансий. Система должна учитывать текущие навыки пользователя и требования вакансий для предложения наиболее релевантных образовательных курсов, способствующих повышению квалификации и карьерному росту.

### Задачи:

### Анализ навыков и вакансий:

 - Разработка алгоритма для анализа требований вакансий и сопоставления их с текущими навыками пользователя.
 - Выделение ключевых навыков для каждой вакансии, таких как технические знания, soft skills, опыт работы с определенными инструментами и технологиями.

### Матчинг навыков пользователя с требованиями вакансии:

 - Создание системы, которая позволяет пользователю вводить свои текущие навыки или загружать профиль, например, из резюме.
 - Сравнение этих навыков с требованиями вакансии через различные метрики и подходы (например, сходство на основе словарей, классификация, векторизация текстов).

### Подбор курсов:

 - Разработка алгоритма для поиска и рекомендации курсов, которые наиболее эффективно компенсируют пробелы в навыках пользователя, исходя из требований вакансии.
 - Курсы могут быть как внешними (например, онлайн-курсы), так и внутренними (предоставляемыми компанией или учебным заведением).

### Прогнозирование роста зарплаты:

 - Внедрение модели прогнозирования, которая будет оценивать, как повышение уровня навыков (с помощью курсов) может повлиять на ожидаемый рост зарплаты после прохождения обучающих курсов.
 - Использование исторических данных для прогнозирования возможного увеличения дохода в зависимости от улучшения навыков.

## Подготовка модели классификации:

**Парсинг вакансий**: Модуль preprocess_vacancy_data загружает данные о вакансии, извлекает ключевые навыки, описание и другие элементы, обрабатывая их для дальнейшего анализа.

**Предобработка текста**: Функция preprocess_text используется для обработки текстовых данных, таких как описание вакансий и резюме кандидатов. Применяется удаление стоп-слов и знаков препинания.

**Обработка данных из CSV**: В функции preprocess_csv_gb происходит загрузка и предварительная обработка данных вакансий, в том числе преобразование текстов в нижний регистр и обработка навыков.

**Сравнение вакансий с резюме**: Модуль использует несколько методов для оценки сходства вакансий с базой данных резюме (например, с помощью расстояния Левенштейна и TF-IDF), чтобы выбрать наиболее подходящие вакансии для кандидатов.

**Создание матрицы сходства**: Для каждой вакансии создается матрица, которая указывает на наличие или отсутствие сходства между требованиями вакансии и резюме кандидатов. Это делается путем сравнения текстов с использованием окна фиксированного размера и порога сходства.


**Моделирование**: В конце производится обучение модели машинного обучения для классификации требований вакансий, используя предварительно обработанные данные.

## Итог обучения:

![binary_clf_res.png](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/binary_clf_res.png)

Как мы видим модель побила необходимый бейзлайн, показала хорошую распознавательную способность.

## Далее подготовка и обучение для модели регрессии:

1. **Загрузка данных**: Загружается CSV-файл, удаляются дубликаты и пропуски в столбце `salary`, а также удаляются HTML-теги из описания вакансий.

2. **Обработка зарплат**: Выполняется подсчет валют в зарплатах и фильтрация по валюте 'RUR'.

3. **Обработка навыков**: Преобразуются навыки в список, фильтруются часто встречающиеся навыки (с частотой ≥ 25) и обновляется столбец `key_skills`.

4. **Преобразование категориальных признаков**: Преобразуются категориальные признаки в числовые с помощью `LabelEncoder`.

5. **Создание бинарных признаков**: Создается датафрейм с бинарными признаками для каждого навыка (0 или 1) и объединяется с основным датафреймом.

6. **Разделение данных**: Данные разделяются на обучающую и тестовую выборки, затем обучается модель `CatBoostRegressor` с подбором гиперпараметров через `GridSearchCV`.

7. **Оценка модели**: Оценка модели на тестовой выборке с расчетом метрик (MAE, MSE, RMSE, R², F1).

![top_features](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/top_features.png)

Самыми важными признаками оказались опыт и регион в котором ищется работа. В то же время признак "работа в команде" тоже является важным, но его вклад отрицательный, так как этот навык говорит, что вам больше нечего написать в резюме (моя интерпретация).

8. **Прогнозирование и визуализация**: Прогнозирование и визуализация метрик с использованием графиков.

![pred_vs_true](https://github.com/ErikWarapaeff/EduMatchAI/blob/hw_3/docs/pictures/pred_vs_true.png)

Мы видим, что модель слабо предсказывает дисперсию, но довольно неплохо предсказывает мат ожидание. Для большей интерпретации модели, добавим классификацию, поделив сбалансированно классы по зарплате. Эвристическими методами, был выбран порог 125000 рублей.

F1 Score: 0.7311355311355311
Mean Absolute Error (MAE): 35202.7891168296
Mean Squared Error (MSE): 2503964657.362611
Root Mean Squared Error (RMSE): 50039.63086756947
Coefficient of Determination (R^2): 0.4645489140419171
F1 Score: 0.7311355311355311

F1 Score равен 0.731, что указывает на хорошую сбалансированность между точностью и полнотой модели в классификации.
Средняя абсолютная ошибка (MAE) составляет 35202.789, что означает, что средняя абсолютная разница между прогнозируемыми и фактическими значениями составляет примерно 35202.789.
Средняя квадратичная ошибка (MSE) равна 2503964657.362, что представляет собой среднюю квадратичную разницу между прогнозируемыми и фактическими значениями.
Корень из средней квадратичной ошибки (RMSE) составляет 50039.631, что является среднеквадратическим отклонением прогнозируемых значений от фактических значений.
Коэффициент детерминации (R^2) равен 0.465, что означает, что модель объясняет примерно 46.5% дисперсии в данных.
Общий вывод: Модель имеет хороший F1 Score и может достаточно точно предсказывать целевую переменную. Однако значения MAE, MSE и RMSE указывают на то, что существует разница между прогнозируемыми и фактическими значениями, и модель может быть дополнительно улучшена. Коэффициент детерминации (R^2) показывает, что модель объясняет менее половины дисперсии в данных, поэтому возможно есть другие факторы, которые не были учтены в модели и влияют на целевую переменную.


9. **Использование модели**: Модель загружается и используется для предсказания на основе заданных переменных (например, тип работы, опыт и регион).


### 3. Подготовка пилота

#### 3.1. Способ оценки пилота
- **Описание**: Для пилота будет разработан сервис на **Streamlit**, который будет запускаться и рассчитывать рекомендации на основе данных, загруженных пользователем. Система должна корректно обрабатывать вводимые данные и выдавать прогнозы или рекомендации в реальном времени.
- **Ответственные**: Product Owner, Data Scientist с группой AB.

#### 3.2. Что считаем успешным пилотом
- **Успех пилота**: Пилот считается успешным, если сервис работает стабильно, выдавая рекомендации на основании загруженных данных, и если его функционал соответствует ожидаемым результатам. Важными критериями успеха являются:
  - Система корректно загружает и обрабатывает данные.
  - Рекомендации соответствуют ожидаемым результатам.
  - Время отклика системы удовлетворяет минимальным требованиям.
- Нет необходимости в дополнительных опросах пользователей для оценки успешности пилота. Если сервис работает правильно, то это уже считается успешным результатом.
- **Ответственные**: Product Owner.

#### 3.3. Подготовка пилота
- **Ресурсы и ограничения**: В процессе подготовки пилота будет произведена оценка вычислительных затрат на модели, чтобы удостовериться в возможности их эффективного запуска. В случае необходимости будет проведен эксперимент с бейзлайном для уточнения вычислительной сложности.
  - **Этапы расчетов**:
    1. Оценка базовой сложности модели с использованием ограниченного объема данных.
    2. Тестирование работы модели в реальном времени с реальными данными.
    3. Уточнение параметров модели в зависимости от полученных результатов и установление ограничений по вычислительной сложности.
  - **Ожидаемые затраты**: Ожидаемые вычислительные ресурсы будут оцениваться с учетом сложности модели и объема данных.
  - **Ответственные**: Data Scientist.

























































